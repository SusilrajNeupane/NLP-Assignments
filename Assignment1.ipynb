{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1214a806",
   "metadata": {},
   "source": [
    "# NLP Using Pyathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef9dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing various libraries\n",
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa94f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing our data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2552302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('flight_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba10a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>CARRIER</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>SCHED_DEP_TIME</th>\n",
       "      <th>ACT_DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>SCHED_ARR_TIME</th>\n",
       "      <th>ACT_ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>G4</td>\n",
       "      <td>PIE</td>\n",
       "      <td>AVL</td>\n",
       "      <td>1511</td>\n",
       "      <td>1533.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1644</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>G4</td>\n",
       "      <td>AUS</td>\n",
       "      <td>SFB</td>\n",
       "      <td>2002</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2335</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>G4</td>\n",
       "      <td>GRI</td>\n",
       "      <td>LAS</td>\n",
       "      <td>1118</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1144</td>\n",
       "      <td>1139.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>G4</td>\n",
       "      <td>AUS</td>\n",
       "      <td>MEM</td>\n",
       "      <td>1643</td>\n",
       "      <td>1726.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1827</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>G4</td>\n",
       "      <td>IND</td>\n",
       "      <td>PIE</td>\n",
       "      <td>858</td>\n",
       "      <td>905.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1107</td>\n",
       "      <td>1119.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>G4</td>\n",
       "      <td>PBG</td>\n",
       "      <td>FLL</td>\n",
       "      <td>1139</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1457</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>-21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>G4</td>\n",
       "      <td>GRR</td>\n",
       "      <td>AZA</td>\n",
       "      <td>1233</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1315</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>G4</td>\n",
       "      <td>BNA</td>\n",
       "      <td>CID</td>\n",
       "      <td>1529</td>\n",
       "      <td>1528.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1701</td>\n",
       "      <td>1654.0</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>G4</td>\n",
       "      <td>SFB</td>\n",
       "      <td>LIT</td>\n",
       "      <td>1800</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1911</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>G4</td>\n",
       "      <td>SFB</td>\n",
       "      <td>DSM</td>\n",
       "      <td>1141</td>\n",
       "      <td>1136.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1330</td>\n",
       "      <td>1327.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>G4</td>\n",
       "      <td>PSC</td>\n",
       "      <td>LAX</td>\n",
       "      <td>1842</td>\n",
       "      <td>1854.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2104</td>\n",
       "      <td>2125.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>G4</td>\n",
       "      <td>RFD</td>\n",
       "      <td>PGD</td>\n",
       "      <td>1639</td>\n",
       "      <td>1652.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>2035.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>G4</td>\n",
       "      <td>TUS</td>\n",
       "      <td>PVU</td>\n",
       "      <td>1528</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1808</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>G4</td>\n",
       "      <td>PIE</td>\n",
       "      <td>FWA</td>\n",
       "      <td>1817</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>2034</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>G4</td>\n",
       "      <td>BLV</td>\n",
       "      <td>LAS</td>\n",
       "      <td>1242</td>\n",
       "      <td>1231.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>1405</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>-25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>G4</td>\n",
       "      <td>LCK</td>\n",
       "      <td>PIE</td>\n",
       "      <td>1021</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1231</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>-13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>G4</td>\n",
       "      <td>TYS</td>\n",
       "      <td>BWI</td>\n",
       "      <td>1348</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1516</td>\n",
       "      <td>1519.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>G4</td>\n",
       "      <td>PGD</td>\n",
       "      <td>BNA</td>\n",
       "      <td>1036</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1132</td>\n",
       "      <td>1109.0</td>\n",
       "      <td>-23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>G4</td>\n",
       "      <td>CID</td>\n",
       "      <td>PIE</td>\n",
       "      <td>1720</td>\n",
       "      <td>1835.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2056</td>\n",
       "      <td>2213.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>G4</td>\n",
       "      <td>AZA</td>\n",
       "      <td>LAS</td>\n",
       "      <td>930</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1036</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>G4</td>\n",
       "      <td>PIE</td>\n",
       "      <td>TYS</td>\n",
       "      <td>1612</td>\n",
       "      <td>1608.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1749</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>G4</td>\n",
       "      <td>LAS</td>\n",
       "      <td>ELP</td>\n",
       "      <td>710</td>\n",
       "      <td>704.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>949</td>\n",
       "      <td>946.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>G4</td>\n",
       "      <td>BOI</td>\n",
       "      <td>LAX</td>\n",
       "      <td>1847</td>\n",
       "      <td>1842.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1958</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>G4</td>\n",
       "      <td>TYS</td>\n",
       "      <td>VPS</td>\n",
       "      <td>1439</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>1459</td>\n",
       "      <td>1451.0</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR  MONTH  DAY CARRIER ORIGIN DEST  SCHED_DEP_TIME  ACT_DEP_TIME  \\\n",
       "0   2019      7   24      G4    PIE  AVL            1511        1533.0   \n",
       "1   2019      7   29      G4    AUS  SFB            2002        2010.0   \n",
       "2   2019      7    7      G4    GRI  LAS            1118        1118.0   \n",
       "3   2019      7    7      G4    AUS  MEM            1643        1726.0   \n",
       "4   2019      7    8      G4    IND  PIE             858         905.0   \n",
       "5   2019      7   20      G4    PBG  FLL            1139        1135.0   \n",
       "6   2019      7   25      G4    GRR  AZA            1233        1240.0   \n",
       "7   2019      7   31      G4    BNA  CID            1529        1528.0   \n",
       "8   2019      7    2      G4    SFB  LIT            1800        1907.0   \n",
       "9   2019      7    2      G4    SFB  DSM            1141        1136.0   \n",
       "10  2019      7    4      G4    PSC  LAX            1842        1854.0   \n",
       "11  2019      7   18      G4    RFD  PGD            1639        1652.0   \n",
       "12  2019      7   19      G4    TUS  PVU            1528        1527.0   \n",
       "13  2019      7   19      G4    PIE  FWA            1817        1813.0   \n",
       "14  2019      7   21      G4    BLV  LAS            1242        1231.0   \n",
       "15  2019      7    2      G4    LCK  PIE            1021        1014.0   \n",
       "16  2019      7   15      G4    TYS  BWI            1348        1344.0   \n",
       "17  2019      7   12      G4    PGD  BNA            1036        1028.0   \n",
       "18  2019      7   16      G4    CID  PIE            1720        1835.0   \n",
       "19  2019      7   17      G4    AZA  LAS             930        1018.0   \n",
       "20  2019      7   21      G4    PIE  TYS            1612        1608.0   \n",
       "21  2019      7    2      G4    LAS  ELP             710         704.0   \n",
       "22  2019      7    2      G4    BOI  LAX            1847        1842.0   \n",
       "23  2019      7    9      G4    TYS  VPS            1439        1430.0   \n",
       "\n",
       "    DEP_DELAY  SCHED_ARR_TIME  ACT_ARR_TIME  ARR_DELAY  \n",
       "0        22.0            1644        1659.0       15.0  \n",
       "1         8.0            2335        2344.0        9.0  \n",
       "2         0.0            1144        1139.0       -5.0  \n",
       "3        43.0            1827        1922.0       55.0  \n",
       "4         7.0            1107        1119.0       12.0  \n",
       "5        -4.0            1457        1436.0      -21.0  \n",
       "6         7.0            1315        1314.0       -1.0  \n",
       "7        -1.0            1701        1654.0       -7.0  \n",
       "8        67.0            1911        2016.0       65.0  \n",
       "9        -5.0            1330        1327.0       -3.0  \n",
       "10       12.0            2104        2125.0       21.0  \n",
       "11       13.0            2019        2035.0       16.0  \n",
       "12       -1.0            1808        1756.0      -12.0  \n",
       "13       -4.0            2034        2023.0      -11.0  \n",
       "14      -11.0            1405        1340.0      -25.0  \n",
       "15       -7.0            1231        1218.0      -13.0  \n",
       "16       -4.0            1516        1519.0        3.0  \n",
       "17       -8.0            1132        1109.0      -23.0  \n",
       "18       75.0            2056        2213.0       77.0  \n",
       "19       48.0            1036        1121.0       45.0  \n",
       "20       -4.0            1749        1748.0       -1.0  \n",
       "21       -6.0             949         946.0       -3.0  \n",
       "22       -5.0            1958        1957.0       -1.0  \n",
       "23       -9.0            1459        1451.0       -8.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57570bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python's utility in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a66b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 2]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sentence = [\"How to change payment method and payment frequency\"]\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit_transform(sentence).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63f6a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK natural language toolkit library\n",
    "# !pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e547e943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Acer/nltk_data'\n    - 'C:\\\\Users\\\\Acer\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\Acer\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Acer\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Acer\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho would have thought that computer programs would be analyzing human sentiments\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m----> 7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Acer/nltk_data'\n    - 'C:\\\\Users\\\\Acer\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\Acer\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Acer\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Acer\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"Who would have thought that computer programs would be analyzing human sentiments\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfab7b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf101fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who', 'would', 'have', 'thought', 'that', 'computer', 'programs', 'would', 'be', 'analyzing', 'human', 'sentiments']\n"
     ]
    }
   ],
   "source": [
    "text = \"Who would have thought that computer programs would be analyzing human sentiments\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d15027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting stop words in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62d9bcfb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db8a6f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who',\n",
       " 'would',\n",
       " 'thought',\n",
       " 'computer',\n",
       " 'programs',\n",
       " 'would',\n",
       " 'analyzing',\n",
       " 'human',\n",
       " 'sentiments']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newtokens = [word for word in tokens if word not in stopwords]\n",
    "newtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6e0d582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# further modify vector by using lemmatization and stemming\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b5cb327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" We can further modify our vector by using lemmatization and stemming, which are\\n techniques that are used to reduce words to their root form. The rationale behind this step\\n is that the imaginary n-dimensional space that we are navigating doesn't need to have\\n separate axes for a word and that word's inflected form (for example, eat and eating don't\\n need to be two separate axes). Therefore, we should reduce each word's inflected form to its\\n root form. However, this approach has its critics because, in many cases, inflected word\\n forms give a different meaning than the root word. For example, the sentences My manager\\n promised me promotion and He is a promising prospect use the inflected form of the root word\\n promise but in entirely different contexts. \""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' We can further modify our vector by using lemmatization and stemming, which are\n",
    " techniques that are used to reduce words to their root form. The rationale behind this step\n",
    " is that the imaginary n-dimensional space that we are navigating doesn't need to have\n",
    " separate axes for a word and that word's inflected form (for example, eat and eating don't\n",
    " need to be two separate axes). Therefore, we should reduce each word's inflected form to its\n",
    " root form. However, this approach has its critics because, in many cases, inflected word\n",
    " forms give a different meaning than the root word. For example, the sentences My manager\n",
    " promised me promotion and He is a promising prospect use the inflected form of the root word\n",
    " promise but in entirely different contexts. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "deafae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who', 'would', 'have', 'thought', 'that', 'computer', 'program', 'would', 'be', 'analyzing', 'human', 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text = \"Who would have thought that computer programs would be analyzing human sentiments\"\n",
    "tokens = word_tokenize(text)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e5f1acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming is similar to lemmatization but  instead of looking up root words in a pre-built\n",
    "#  dictionary, it defines some rules based on which words are reduced to their root form. For\n",
    "#  example, it has a rule that states that any word with ing as a suffix will be reduced by\n",
    "#  removing the suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0e16593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50d70b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'would', 'have', 'thought', 'that', 'comput', 'program', 'would', 'be', 'analyz', 'human', 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Who would have thought that computer programs would be analyzing human sentiments\"\n",
    "tokens=word_tokenize(text.lower())\n",
    "ps = PorterStemmer()\n",
    "tokens=[ps.stem(word) for word in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1de497a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c31d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f84ef48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('your', 'PRP$')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"your\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4889057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beautiful', 'NN')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"beautiful\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5a22a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eat', 'NN')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"eat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5fcc997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Usain', 'NN')],\n",
       " [('Bolt', 'NN')],\n",
       " [('is', 'VBZ')],\n",
       " [('the', 'DT')],\n",
       " [('fastest', 'JJS')],\n",
       " [('runner', 'NN')],\n",
       " [('in', 'IN')],\n",
       " [('the', 'DT')],\n",
       " [('world', 'NN')]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Usain Bolt is the fastest runner in the world\"\n",
    "tokens = word_tokenize(text)\n",
    "[nltk.pos_tag([word]) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c21b20de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34c59c90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aeb6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b5c23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8842402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "954409c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07a58adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.6)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "TextBlob(\"I love pizza\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6107e7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=1.0, subjectivity=1.0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"The weather is excellent\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190bbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f1f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f9c54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4587cfe1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextBlob' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [55], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m languages \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzh-CN\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m language \u001b[38;5;129;01min\u001b[39;00m languages:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho knew translation could be fun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m(to\u001b[38;5;241m=\u001b[39mlanguage))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextBlob' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "languages = ['fr','zh-CN','hi']\n",
    "for language in languages:\n",
    "    print(TextBlob(\"Who knew translation could be fun\").translate(to=language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56a57427",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextBlob' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow you doing today\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m(to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextBlob' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "TextBlob(\"How you doing today\").translate(to='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1c744c2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextBlob' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [67], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow you doing today\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m blob \u001b[38;5;241m=\u001b[39m TextBlob(text)\n\u001b[1;32m----> 5\u001b[0m translated_blob \u001b[38;5;241m=\u001b[39m \u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m(to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(translated_blob)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextBlob' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"How you doing today\"\n",
    "blob = TextBlob(text)\n",
    "translated_blob = blob.translate(to='hi')\n",
    "\n",
    "print(translated_blob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1e079e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextBlob' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [68], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m languages \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzh-CN\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m language \u001b[38;5;129;01min\u001b[39;00m languages:\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho knew translation could be fun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m(to\u001b[38;5;241m=\u001b[39mlanguage))\n\u001b[0;32m      5\u001b[0m TextBlob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe global economy is expected to grow this year\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtags\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextBlob' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "languages = ['fr','zh-CN','hi']\n",
    "for language in languages:\n",
    "    print(TextBlob(\"Who knew translation could be fun\").translate(to=language))\n",
    "    \n",
    "TextBlob(\"The global economy is expected to grow this year\").tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9805a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "     -------------------------------------- 55.1/55.1 kB 954.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\acer\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2022.12.7)\n",
      "Collecting hstspreload\n",
      "  Downloading hstspreload-2024.2.1-py3-none-any.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 204.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\acer\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Collecting chardet==3.*\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "     ------------------------------------ 133.4/133.4 kB 151.4 kB/s eta 0:00:00\n",
      "Collecting idna==2.*\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "     -------------------------------------- 58.8/58.8 kB 163.5 kB/s eta 0:00:00\n",
      "Collecting rfc3986<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting httpcore==0.9.*\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "     -------------------------------------- 42.6/42.6 kB 189.5 kB/s eta 0:00:00\n",
      "Collecting h11<0.10,>=0.8\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "     -------------------------------------- 53.6/53.6 kB 252.1 kB/s eta 0:00:00\n",
      "Collecting h2==3.*\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "     -------------------------------------- 65.0/65.0 kB 125.4 kB/s eta 0:00:00\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Running setup.py install for googletrans: started\n",
      "  Running setup.py install for googletrans: finished with status 'done'\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.2.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: googletrans is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78bac265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne: कसलाई अनुवाद थाहा थियो\n",
      "zh-CN: 谁知道翻译可能很有趣\n",
      "fr: qui savait que la traduction pouvait être amusante\n",
      "hi: कौन जानता था कि अनुवाद मजेदार हो सकता है\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from googletrans import Translator\n",
    "\n",
    "languages = ['ne', 'zh-CN', 'fr','hi']\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "for language in languages:\n",
    "    translated_text = translator.translate(\"who knew translation could be fun\", dest=language).text\n",
    "    print(f\"{language}: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "359ef5c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Translator.translate() got an unexpected keyword argument 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m translator \u001b[38;5;241m=\u001b[39m Translator()\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow you doing today\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m translated_blob \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(translated_blob)\n",
      "\u001b[1;31mTypeError\u001b[0m: Translator.translate() got an unexpected keyword argument 'to'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "# from textblob.translate import Translator\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "text = \"How you doing today\"\n",
    "translated_blob = translator.translate(to='hi')\n",
    "\n",
    "print(translated_blob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a76818c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('global', 'JJ'),\n",
       " ('economy', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('expected', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('grow', 'VB'),\n",
       " ('this', 'DT'),\n",
       " ('year', 'NN')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " TextBlob(\"The global economy is expected to grow this year\").tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21798b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9485e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f97409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d97ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e603727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f79bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b943856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.4927}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores(\"This book is very good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aadef9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The compound score is what\\n we are interested in. Any score greater\\n than 0.05 is considered positive, while less\\n than -0.05\\n is considered negative '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" The compound score is what\n",
    " we are interested in. Any score greater\n",
    " than 0.05 is considered positive, while less\n",
    " than -0.05\n",
    " is considered negative \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4fc2064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.259, 'neu': 0.741, 'pos': 0.0, 'compound': -0.2732}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores(\"I don't think I get low marks\")\n",
    "# Above line should give result more towards positive but the result is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea04f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2618e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89cdb10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6006b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35ce43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   WEB SCRAPING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad807981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requsets\n",
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1afeaf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7219b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf912cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2adc200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f36de96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "prices = []\n",
    "ratings = []\n",
    "url ='https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops'\n",
    "request = requests.get(url)\n",
    "soup = BeautifulSoup(request.text,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29d154ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in soup.find_all('div',{'class':'col-sm-4 col-lg-4 col-md-4'}):\n",
    "    for pr in product.find_all('div',{'class':'caption'}):\n",
    "        for p in pr.find_all('h4',{'class':'pull-right price'}):\n",
    "            prices.append(p.text)\n",
    "        for title in pr.find_all('a',{'title'}):\n",
    "            titles.append(title.get('title'))\n",
    "    for rt in product.find_all('div',{'class':'ratings'}):\n",
    "        ratings.append(len(rt.find_all('span',{'class':'glyphicon glyphicon-star'})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c0cb609",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.DataFrame(zip(titles,prices,ratings),columns=['Titles','Prices','Ratings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df7fbd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Prices</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Titles, Prices, Ratings]\n",
       "Index: []"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ba86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00a4be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install pandas\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "titles = []\n",
    "prices = []\n",
    "ratings = []\n",
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops'\n",
    "request = requests.get(url)\n",
    "soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "for product in soup.find_all('div', {'class': 'col-sm-4 col-lg-4 col-md-4'}):\n",
    "    for pr in product.find_all('div', {'class': 'caption'}):\n",
    "        for p in pr.find_all('h4', {'class': 'pull-right price'}):\n",
    "            prices.append(p.text)\n",
    "        for title in pr.find_all('a' , {'title'}):\n",
    "            titles.append(title.get('title'))\n",
    "    for rt in product.find_all('div', {'class': 'ratings'}):\n",
    "        ratings.append(len(rt.find_all('span', {'class': 'glyphicon glyphicon-star'})))\n",
    "\n",
    "\n",
    "            \n",
    "product_df = pd.DataFrame(zip(titles,prices,ratings), columns =['Titles', 'Prices', 'Ratings'])  \n",
    "product_df.head()\n",
    "product_df.to_csv(\"ecommerce.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf203d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
